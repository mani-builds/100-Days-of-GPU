* Vinay's advice (GPU performance architecht @ AMD)

Focus on one of the four:
Ref: [[https://grok.com/c/63efba58-6d46-481b-8c1c-04e49533ad23][Grok chat]]
1. CUDA-level programming (single GPU, multi GPU)
   OSS Repos:
    [[https://github.com/NVIDIA/CUDALibrarySamples][CUDA Library Samples]] : Samples for cuBLAS, cuFFT, etc. Contribute new use cases or optimizations for single/multi-GPU scenarios. Active.
    [[https://github.com/NVIDIA/cccl][CUDA Core Compute Libraries]] : CUDA Core Compute Libraries (Thrust, CUB, libcudacx). Ideal for parallel algorithm improvements, warp-level primitives, or multi-GPU extensions. Very active and also has a discord.
    [[https://github.com/NVIDIA/open-gpu-kernel-modules][Open GPU Kernel Modules]] : Open-source NVIDIA Linux GPU kernel modules. Contribute to low-level driver fixes, multi-GPU support, or performance patches
    Other OSS repos:
        https://github.com/rapidsai/cuml [cuML]: C++,cuda,Python, Very active. ML algorithms on GPU.
        https://github.com/cupy/cupy : Python mostly, active. NumPy and SciPy for GPUs.
        https://github.com/rapidsai/raft : Cuda mostly, active. CUDA accelerated algos for ML.
        https://github.com/hpcaitech/ColossalAI : Python mostly, not super active. Data parallelism in AI, tools for distributed ML


2. Inference or training platform optimization (mostly on multi GPU on a higher level like vllm, tensor rt,etc)
   OSS Repos:
    [[https://github.com/vllm-project/vllm][vLLM]] : High-throughput LLM inference with PagedAttention; contribute to multi-GPU serving, new quant methods, or integrations. Very active
    [[https://github.com/NVIDIA/TensorRT/issues][TensorRT]] : NVIDIA's LLM inference optimizer; add custom kernels, multi-GPU/node support, or quantization plugins
    [[https://github.com/sgl-project][SGLang]] : Fast serving framework; optimizations for MoE/VLM models on multi-GPU
    [[https://github.com/triton-inference-server/server][Triton Inference Server]] : Modular inference serving; integrate with vLLM/TensorRT, optimize backends for multi-GPU
    [[https://github.com/InternLM/lmdeploy][InternLM/Imdeploy]] : Fast LLM deployment; contribute to turbo-mode inference or multi-GPU pipelines
3. Specific focus on compilers, graph optimization (CUDA computational graphs), llvm ir and similar
4. General deep learning engineers on algorithmic level with CUDA experience
   OSS Repos:
    [[https://github.com/pytorch/pytorch][pytorch/pytorch]]: Core framework; add CUDA kernels, distributed training, or new nn modules.
    [[https://github.com/jax-ml/jax][google/jax]]: NumPy-like with XLA/CUDA; custom ops or parallel primitives.
    [[https://github.com/huggingface/transformers][huggingface/transformers]]: Model hub; CUDA-optimized implementations or new architectures.
    deepmind/jmp or Flax/Haiku ecosystems (part of JAX): Algorithmic layers with CUDA.


*** Notes on the above roles:
1. CUDA-level programming (single GPU, multi GPU)
   This role focuses on low-level GPU programming to accelerate compute-intensive tasks in ML/AI, such as writing custom kernels for operations not efficiently handled by higher-level libraries. Engineers here optimize memory access, parallelism, and synchronization to maximize GPU utilization. For single-GPU, it's about fine-grained kernel optimizations; for multi-GPU, it involves data parallelism, model parallelism, and communication across devices. This is hands-on, performance-critical work often in research or high-performance computing environments, like speeding up custom neural network layers or simulations.
Required Tech Stack:

Programming Languages: C++ (core for CUDA extensions), Python (for integration with ML frameworks).
Core NVIDIA Tools: CUDA Toolkit (including nvcc compiler, CUDA C/C++), PTX (Parallel Thread Execution assembly).
Libraries for Compute: cuBLAS (linear algebra), cuDNN (deep neural networks), cuFFT (Fourier transforms), Thrust (parallel algorithms).
Multi-GPU Specific: NCCL (collective communications), NVLink/NVSwitch for interconnects, MPI or Horovod for distributed setups.
Debugging/Profiling: Nsight Compute/Systems, CUDA-GDB, nvprof.
Integration: PyTorch/TensorFlow CUDA extensions (e.g., via torch.utils.cpp_extension or tf.custom_ops).

2. Inference or training platform optimization (mostly on multi GPU on a higher level like vllm, tensor rt,etc)
   This role involves optimizing ML platforms for efficient training or inference at scale, focusing on system-level improvements rather than kernel writing. Engineers here build pipelines that handle large models (e.g., LLMs) across multiple GPUs, reducing latency, throughput bottlenecks, and costs. For training, it's about distributed strategies; for inference, it's quantization, batching, and serving. This is common in production environments like cloud services or edge deployment, ensuring models run reliably at high volumes.
Required Tech Stack:

ML Frameworks: PyTorch (with DistributedDataParallel or FSDP), TensorFlow (with tf.distribute), JAX (for scalable training).
Optimization Tools: vLLM (for LLM inference), TensorRT (NVIDIA's inference optimizer), ONNX Runtime (model export/optimization), Hugging Face Optimum.
Serving/Inference: Triton Inference Server, FastAPI or BentoML for APIs, TorchServe.
Distributed Systems: Ray (for scaling workflows), Dask or Horovod (data parallelism), Kubernetes or Slurm for cluster management.
Performance Enhancers: Mixed precision (AMP/bfloat16), model quantization (INT8/FP8), CUDA Graphs for kernel fusion.
Monitoring: Prometheus/Grafana, NVIDIA DCGM (Data Center GPU Manager).

3. Specific focus on compilers, graph optimization (CUDA computational graphs), llvm ir and similar
   This specialized role centers on building or enhancing ML compilers and optimizers that transform high-level model graphs into efficient low-level code tailored to hardware. Engineers optimize computational graphs for fusion, scheduling, and code generation, often targeting GPUs. This involves intermediate representations (IR) like LLVM IR to enable cross-hardware portability and performance. It's research-heavy, seen in teams developing custom backends for frameworks, like improving compilation for novel architectures.
Required Tech Stack:

Compiler Frameworks: LLVM (core IR and tooling), MLIR (Multi-Level IR for dialect-based optimizations), Apache TVM (end-to-end ML compiler stack).
Graph Optimizers: PyTorch TorchScript/JIT, TensorFlow XLA (Accelerated Linear Algebra), Glow (Graph Lowering Optimizer).
CUDA-Specific: CUDA Graphs API (for capturing and replaying kernels), PTX/SASS assembly integration.
Languages/Tools: C++ (for compiler backends), Python (scripting optimizations), Halide or Polyhedral models for loop optimizations.
Advanced: Tensor Comprehensions, Relay IR (in TVM), ONNX for graph interchange.
Testing/Debugging: LLVM tools like llc (LLVM compiler), opt (optimizer passes), Clang for frontend.

4. General deep learning engineers on algorithmic level with CUDA experience
This role is broader, emphasizing the design and implementation of DL algorithms and models, with CUDA knowledge for when high-level APIs fall short (e.g., custom operators). Engineers focus on architecture innovation, experimentation, and prototyping, but can drop to CUDA for optimizations like custom activations or efficient data loaders. It's algorithmic-centric, balancing theory with practical implementation, common in R&D teams where models need both novelty and efficiency.
Required Tech Stack:

DL Frameworks: PyTorch (preferred for flexibility), TensorFlow/Keras, JAX (for autodiff and composability).
Algorithmic Tools: torch.nn/torch.optim (model building), NumPy/SciPy (data manipulation), Hugging Face Transformers/Diffusers (pre-trained models).
CUDA Integration: torch.cuda for device management, writing custom CUDA kernels via torch.utils.cpp_extension, cuDNN for convolutions/RNNs.
Experimentation: Weights & Biases or MLflow (tracking), Jupyter/Colab for prototyping.
Scaling Basics: DataParallel/ModelParallel in PyTorch, basic NCCL for multi-GPU experiments.
General: Git for version control, Docker for reproducibility, basic profiling with torch.profiler.

** General tips
- Open source is very important
- Tailor projects to the roles you are looking for
- Some jobs cover compiler, llvm ir
- Inference roles: vllm, tensor rt

** His interview (Varies from role to role)
- CUDA/HIP programming
- C++ LeetCode and OOPs concepts
- Numpy
- PyTorch
- Python
  These roles are typically called ML Infrastructure/ ML Platform Engineers / GPU Performance engineers

* ML/AI infrastructure engineer, ML/AI platform engineer
Source: https://huyenchip.com/ml-interviews-book/contents/1.1.3.4-other-technical-roles-in-ml-production.html

Because ML is resource-intensive, it relies on infrastructures that scale. Companies with mature ML pipelines often have infrastructure teams to help them build out the infrastructure for ML. Valuable skills for ML infrastructure/platform engineers include familiarity with parallelism, distributed computing, and low-level optimization.

These skills are hard to learn and take time to master, so companies prefer hiring engineers who are already skillful in this and train them in ML. If you are a rare breed that knows both systems and ML, you’ll be in demand.
*** Tech Stack:
ML Algorithms, Systems
| Category                | Technology/Tool                     | Purpose                                  |
|-------------------------|-------------------------------------|------------------------------------------|
| Programming Languages   | Python                              | Core scripting for automation, ML pipelines, and infrastructure scripting |
|                         | Go                                  | High-performance services, distributed systems, and tooling (e.g., Kubernetes operators) |
|                         | C++/Rust (optional)                 | Low-level optimizations and performance-critical components |
| Distributed Computing   | Apache Spark                        | Scalable data processing and distributed ML training |
|                         | Ray                                 | Distributed computing for ML workloads, including training and serving |
|                         | Dask                                | Parallel computing on Python objects for scalable ML |
| ML Frameworks           | TensorFlow                          | Distributed training and serving infrastructure |
|                         | PyTorch                             | Flexible distributed training with TorchServe for deployment |
|                         | Kubeflow                            | End-to-end ML platform on Kubernetes for pipelines and scaling |
| Infrastructure Tools    | Kubernetes                          | Container orchestration for scalable ML deployments |
|                         | Docker                              | Containerization for reproducible ML environments |
|                         | Apache Airflow                      | Workflow orchestration for ML pipelines |
| Cloud Platforms         | AWS (SageMaker, EKS)                | Managed ML infrastructure, compute, and storage |
|                         | Google Cloud (Vertex AI, GKE)       | AI/ML platform services and Kubernetes Engine |
|                         | Azure (AKS, ML Studio)              | Kubernetes and managed ML ops services |
| Monitoring & Optimization| Prometheus + Grafana                | Metrics monitoring for distributed systems |
|                         | MLflow                              | Experiment tracking and model lifecycle management |
|                         | Weights & Biases (W&B)              | Hyperparameter tuning and resource optimization |
| Version Control & CI/CD | Git + GitHub Actions                | Code versioning and automated ML pipeline deployments |
|                         | Jenkins/Terraform                   | CI/CD pipelines and infrastructure as code for ML infra |
| Others                  | NVIDIA CUDA                         | GPU acceleration for low-level ML optimizations |
|                         | Helm                                | Package management for Kubernetes-based ML platforms |



* People to follow in this space
 - Maharshi (https://x.com/mrsiipa): The GOAT
 - Elliot Arledge (https://x.com/elliotarledge): The cuda guy

* Cuda companies
** Role: GPU Kernel Engineer (Baseten)
  Core Engineering Responsibilities

    Design and implement high-performance GPU kernels for key ML operations, including matrix multiplications, attention mechanisms, and mixture-of-experts routing
    Write and optimize code using CUDA, PTX assembly, and architecture-specific techniques
    Apply advanced performance optimization methods such as memory coalescing, warp-level programming, tensor core acceleration, and compute/memory overlap

Performance & Innovation

    Implement cutting-edge features like quantization (FP8/FP4), sparsity, and compute/communication overlap
    Identify and resolve performance bottlenecks using tools like Nsight Systems, Nsight Compute, and Torch Profiler
    Collaborate with research teams to productionize theoretical advancements

Impact & Collaboration

    Contribute to internal and open-source GPU libraries

Requirements

    1–5 years of experience in CUDA development
    Strong understanding of GPU architecture and programming paradigms:
        Memory hierarchy (global, shared, registers, L1/L2 cache)
        Thread/block/grid organization
        Synchronization techniques and race condition mitigation
    Proficient in C++ and GPU performance profiling tools
    Knowledge of:
        CUDA C++ API
        Memory access patterns and bandwidth optimization
        Numerical precision and quantization strategies
        Modern GPU features (e.g., tensor cores, async operations)

NICE TO HAVE

    Experience with Transformer models and attention optimization (e.g., Flash Attention)
    Familiarity with GPU kernel libraries: Cutlass, Triton, Thrust, CUB
    Background in GEMM tuning and distributed/multi-GPU compute
    Contributions to open-source GPU projects

** Cuda Kernel Engineer (Pragmatike)
What Youll Do

    Design, implement, and optimize custom CUDA kernels for NVIDIA GPUs, with a focus on maximizing occupancy, memory throughput, and warp efficiency.
    Profile GPU workloads using tools such as Nsight Compute, Nsight Systems, nvprof, and CUDA‐MEMCHECK.
    Analyze and eliminate performance bottlenecks including warp divergence, uncoalesced memory access, register pressure, and PCIe transfer overhead.
    Improve GPU memory pipelines (global, shared, L2, texture memory) and ensure proper memory coalescing.
    Collaborate closely with AI systems, model acceleration, and backend distributed systems teams.
    Contribute to GPU architecture decisions, kernel libraries, and internal performance-engineering best practices.


What Were Looking For

    Proven track record building NVIDIA CUDA kernels from scratchnot just calling existing libraries.
    Strong ability to optimize kernels (tiling strategies, occupancy tuning, shared memory design, warp scheduling).
    Deep understanding of CUDA threads, warps, blocks, and grids, GPU memory hierarchy and memory coalescing, as well as warp divergence (how to detect, analyze, and mitigate it)
    Experience diagnosing PCIe bottlenecks and optimizing host-device transfers (pinned memory, streams, batching, overlap).
    Familiarity with C++, CUDA runtime APIs, and GPU debugging/profiling tooling.


Bonus Points

    Experience with multi-GPU or distributed GPU systems (NCCL, NVLink, MIG).
    Background in GPU acceleration for ML frameworks or HPC workloads.
    Knowledge of model inference optimization (TensorRT, CUDA Graphs, CUTLASS).
    Exposure to compiler-level optimization or PTX/SASS analysis.
** AI Performance Engineer (Parasail)
Responsibilities

    Add support for new LLMs, working across the stack from low-level GPU kernels to Kubernetes-based deployments.
    Contribute to cutting-edge open-source LLM engines such as vLLM or SGLang to extend their capabilities and performance (e.g. use Python technologies to improve API servers or request schedulers).
    Operate closer to the hardware, focusing on building and integrating solutions to boost performance and hardware utilization. For example, improve attention backends like FlashAttention or FlashInfer by contributing to their development and optimization, or by integrating their solutions into vLLM.
    Improve LLM performance using advanced algorithmic solutions such as speculative decoding, quantization, or other state-of-the-art techniques. Understand the impact of such techniques in model quality.

Qualifications

    Expertise in GPU computing, including low-level platforms such as CUDA, ROCm, XLA, PyTorch, Jax, etc.
    Background in performance analysis and optimization of AI/HPC workloads (e.g. profiling or theoretical analysis of Flops and bandwidth).
    Experience in writing GPU kernels using technologies like CUDA, CUTLASS, Triton.
    Strength in Python and C++.
    Demonstrated contributions to open-source projects. Contributions to inference engines such as vLLM is a strong plus.
    A production-oriented mindset emphasizing robust, scalable code suitable for enterprise-grade applications.
    A relentless curiosity about cutting-edge AI technologies combined with a passion for solving complex problems.

** How to search?
Search for inference or model optimization roles. There is still a demand for HPC developers but its a smaller industry compared to AI/ML, and usually industry specific.

* Mohan's advice (Mavayya's friend)
- Study the inference stacks that are being deployed on GPUs, TPUs.
- Understand LLM inferencing workloads and how to to optimization and performance characterizaton. (you should be able to write optimized kernels)
- Look at arxiv papers that do performance characterizaton of inference stacks.
- Study the core tools and tech stacks they are working with.
- Companies to lookout for Coreweave, Crusoe and their competitors.
