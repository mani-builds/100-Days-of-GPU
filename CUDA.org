* Vinay's advice (GPU performance architecht @ AMD)
Focus on one of the four:
Ref: [[https://grok.com/c/63efba58-6d46-481b-8c1c-04e49533ad23][Grok chat]]
1. CUDA-level programming (single GPU, multi GPU)
   OSS Repos:
    [[https://github.com/NVIDIA/CUDALibrarySamples][CUDA Library Samples]] : Samples for cuBLAS, cuFFT, etc. Contribute new use cases or optimizations for single/multi-GPU scenarios. Active.
    [[https://github.com/NVIDIA/cccl][CUDA Core Compute Libraries]] : CUDA Core Compute Libraries (Thrust, CUB, libcudacx). Ideal for parallel algorithm improvements, warp-level primitives, or multi-GPU extensions. Very active and also has a discord.
    [[https://github.com/NVIDIA/open-gpu-kernel-modules][Open GPU Kernel Modules]] : Open-source NVIDIA Linux GPU kernel modules. Contribute to low-level driver fixes, multi-GPU support, or performance patches
2. Inference or training platform optimization (mostly on multi GPU on a higher level like vllm, tensor rt,etc)
   OSS Repos:
    [[https://github.com/vllm-project/vllm][vLLM]] : High-throughput LLM inference with PagedAttention; contribute to multi-GPU serving, new quant methods, or integrations. Very active
    [[https://github.com/NVIDIA/TensorRT/issues][TensorRT]] : NVIDIA's LLM inference optimizer; add custom kernels, multi-GPU/node support, or quantization plugins
    [[https://github.com/sgl-project][SGLang]] : Fast serving framework; optimizations for MoE/VLM models on multi-GPU
    [[https://github.com/triton-inference-server/server][Triton Inference Server]] : Modular inference serving; integrate with vLLM/TensorRT, optimize backends for multi-GPU
    [[https://github.com/InternLM/lmdeploy][InternLM/Imdeploy]] : Fast LLM deployment; contribute to turbo-mode inference or multi-GPU pipelines
3. Specific focus on compilers, graph optimization (CUDA computational graphs), llvm ir and similar
4. General deep learning engineers on algorithmic level with CUDA experience
   OSS Repos:
    [[https://github.com/pytorch/pytorch][pytorch/pytorch]]: Core framework; add CUDA kernels, distributed training, or new nn modules.
    [[https://github.com/jax-ml/jax][google/jax]]: NumPy-like with XLA/CUDA; custom ops or parallel primitives.
    [[https://github.com/huggingface/transformers][huggingface/transformers]]: Model hub; CUDA-optimized implementations or new architectures.
    deepmind/jmp or Flax/Haiku ecosystems (part of JAX): Algorithmic layers with CUDA.
*** Notes on the above roles:
   This role focuses on low-level GPU programming to accelerate compute-intensive tasks in ML/AI, such as writing custom kernels for operations not efficiently handled by higher-level libraries. Engineers here optimize memory access, parallelism, and synchronization to maximize GPU utilization. For single-GPU, it's about fine-grained kernel optimizations; for multi-GPU, it involves data parallelism, model parallelism, and communication across devices. This is hands-on, performance-critical work often in research or high-performance computing environments, like speeding up custom neural network layers or simulations.
Required Tech Stack:

Programming Languages: C++ (core for CUDA extensions), Python (for integration with ML frameworks).
Core NVIDIA Tools: CUDA Toolkit (including nvcc compiler, CUDA C/C++), PTX (Parallel Thread Execution assembly).
Libraries for Compute: cuBLAS (linear algebra), cuDNN (deep neural networks), cuFFT (Fourier transforms), Thrust (parallel algorithms).
Multi-GPU Specific: NCCL (collective communications), NVLink/NVSwitch for interconnects, MPI or Horovod for distributed setups.
Debugging/Profiling: Nsight Compute/Systems, CUDA-GDB, nvprof.
Integration: PyTorch/TensorFlow CUDA extensions (e.g., via torch.utils.cpp_extension or tf.custom_ops).
2. Inference or training platform optimization (mostly on multi GPU on a higher level like vllm, tensor rt,etc)
   This role involves optimizing ML platforms for efficient training or inference at scale, focusing on system-level improvements rather than kernel writing. Engineers here build pipelines that handle large models (e.g., LLMs) across multiple GPUs, reducing latency, throughput bottlenecks, and costs. For training, it's about distributed strategies; for inference, it's quantization, batching, and serving. This is common in production environments like cloud services or edge deployment, ensuring models run reliably at high volumes.
Required Tech Stack:

ML Frameworks: PyTorch (with DistributedDataParallel or FSDP), TensorFlow (with tf.distribute), JAX (for scalable training).
Optimization Tools: vLLM (for LLM inference), TensorRT (NVIDIA's inference optimizer), ONNX Runtime (model export/optimization), Hugging Face Optimum.
Serving/Inference: Triton Inference Server, FastAPI or BentoML for APIs, TorchServe.
Distributed Systems: Ray (for scaling workflows), Dask or Horovod (data parallelism), Kubernetes or Slurm for cluster management.
Performance Enhancers: Mixed precision (AMP/bfloat16), model quantization (INT8/FP8), CUDA Graphs for kernel fusion.
Monitoring: Prometheus/Grafana, NVIDIA DCGM (Data Center GPU Manager).

3. Specific focus on compilers, graph optimization (CUDA computational graphs), llvm ir and similar
   This specialized role centers on building or enhancing ML compilers and optimizers that transform high-level model graphs into efficient low-level code tailored to hardware. Engineers optimize computational graphs for fusion, scheduling, and code generation, often targeting GPUs. This involves intermediate representations (IR) like LLVM IR to enable cross-hardware portability and performance. It's research-heavy, seen in teams developing custom backends for frameworks, like improving compilation for novel architectures.
Required Tech Stack:

Compiler Frameworks: LLVM (core IR and tooling), MLIR (Multi-Level IR for dialect-based optimizations), Apache TVM (end-to-end ML compiler stack).
Graph Optimizers: PyTorch TorchScript/JIT, TensorFlow XLA (Accelerated Linear Algebra), Glow (Graph Lowering Optimizer).
CUDA-Specific: CUDA Graphs API (for capturing and replaying kernels), PTX/SASS assembly integration.
Languages/Tools: C++ (for compiler backends), Python (scripting optimizations), Halide or Polyhedral models for loop optimizations.
Advanced: Tensor Comprehensions, Relay IR (in TVM), ONNX for graph interchange.
Testing/Debugging: LLVM tools like llc (LLVM compiler), opt (optimizer passes), Clang for frontend.

4. General deep learning engineers on algorithmic level with CUDA experience
This role is broader, emphasizing the design and implementation of DL algorithms and models, with CUDA knowledge for when high-level APIs fall short (e.g., custom operators). Engineers focus on architecture innovation, experimentation, and prototyping, but can drop to CUDA for optimizations like custom activations or efficient data loaders. It's algorithmic-centric, balancing theory with practical implementation, common in R&D teams where models need both novelty and efficiency.
Required Tech Stack:

DL Frameworks: PyTorch (preferred for flexibility), TensorFlow/Keras, JAX (for autodiff and composability).
Algorithmic Tools: torch.nn/torch.optim (model building), NumPy/SciPy (data manipulation), Hugging Face Transformers/Diffusers (pre-trained models).
CUDA Integration: torch.cuda for device management, writing custom CUDA kernels via torch.utils.cpp_extension, cuDNN for convolutions/RNNs.
Experimentation: Weights & Biases or MLflow (tracking), Jupyter/Colab for prototyping.
Scaling Basics: DataParallel/ModelParallel in PyTorch, basic NCCL for multi-GPU experiments.
General: Git for version control, Docker for reproducibility, basic profiling with torch.profiler.

** General tips
- Open source is very important
- Tailor projects to the roles you are looking for
- Some jobs cover compiler, llvm ir
- Inference roles: vllm, tensor rt

** His interview (Varies from role to role)
- CUDA/HIP programming
- C++ LeetCode and OOPs concepts
- Numpy
- PyTorch
- Python
  These roles are typically called ML Infrastructure/ ML Platform Engineers / GPU Performance engineers

* ML/AI infrastructure engineer, ML/AI platform engineer
Source: https://huyenchip.com/ml-interviews-book/contents/1.1.3.4-other-technical-roles-in-ml-production.html

Because ML is resource-intensive, it relies on infrastructures that scale. Companies with mature ML pipelines often have infrastructure teams to help them build out the infrastructure for ML. Valuable skills for ML infrastructure/platform engineers include familiarity with parallelism, distributed computing, and low-level optimization.

These skills are hard to learn and take time to master, so companies prefer hiring engineers who are already skillful in this and train them in ML. If you are a rare breed that knows both systems and ML, youâ€™ll be in demand.
*** Tech Stack:
ML Algorithms, Systems
| Category                | Technology/Tool                     | Purpose                                  |
|-------------------------|-------------------------------------|------------------------------------------|
| Programming Languages   | Python                              | Core scripting for automation, ML pipelines, and infrastructure scripting |
|                         | Go                                  | High-performance services, distributed systems, and tooling (e.g., Kubernetes operators) |
|                         | C++/Rust (optional)                 | Low-level optimizations and performance-critical components |
| Distributed Computing   | Apache Spark                        | Scalable data processing and distributed ML training |
|                         | Ray                                 | Distributed computing for ML workloads, including training and serving |
|                         | Dask                                | Parallel computing on Python objects for scalable ML |
| ML Frameworks           | TensorFlow                          | Distributed training and serving infrastructure |
|                         | PyTorch                             | Flexible distributed training with TorchServe for deployment |
|                         | Kubeflow                            | End-to-end ML platform on Kubernetes for pipelines and scaling |
| Infrastructure Tools    | Kubernetes                          | Container orchestration for scalable ML deployments |
|                         | Docker                              | Containerization for reproducible ML environments |
|                         | Apache Airflow                      | Workflow orchestration for ML pipelines |
| Cloud Platforms         | AWS (SageMaker, EKS)                | Managed ML infrastructure, compute, and storage |
|                         | Google Cloud (Vertex AI, GKE)       | AI/ML platform services and Kubernetes Engine |
|                         | Azure (AKS, ML Studio)              | Kubernetes and managed ML ops services |
| Monitoring & Optimization| Prometheus + Grafana                | Metrics monitoring for distributed systems |
|                         | MLflow                              | Experiment tracking and model lifecycle management |
|                         | Weights & Biases (W&B)              | Hyperparameter tuning and resource optimization |
| Version Control & CI/CD | Git + GitHub Actions                | Code versioning and automated ML pipeline deployments |
|                         | Jenkins/Terraform                   | CI/CD pipelines and infrastructure as code for ML infra |
| Others                  | NVIDIA CUDA                         | GPU acceleration for low-level ML optimizations |
|                         | Helm                                | Package management for Kubernetes-based ML platforms |



* People to follow in this space
 - Maharshi (https://x.com/mrsiipa): The GOAT
 - Elliot Arledge (https://x.com/elliotarledge): The cuda guy

* Cuda companies
- AsseblyAI
It's job description:
    Strong expertise in the Python ecosystem and major ML frameworks (PyTorch, JAX).
    Experience with lower-level programming (C++ or Rust preferred).
    Deep understanding of GPU acceleration (CUDA, profiling, kernel-level optimization); TPU experience is a strong plus.
    Proven ability to accelerate deep learning workloads using compiler frameworks, graph optimizations, and parallelization strategies.
    Solid understanding of the deep learning lifecycle: model design, large-scale training, data processing pipelines, and inference deployment.
    Strong debugging, profiling, and optimization skills in large-scale distributed environments.
    Excellent communication and collaboration skills, with the ability to clearly prioritize and articulate impact-driven technical solutions.

* Mohan's advice (Mavayya's friend)
- Study the inference stacks that are being deployed on GPUs, TPUs.
- Understand LLM inferencing workloads and how to to optimization and performance characterizaton. (you should be able to write optimized kernels)
- Look at arxiv papers that do performance characterizaton of inference stacks.
- Study the core tools and tech stacks they are working with.
- Companies to lookout for Coreweave, Crusoe and their competitors.
